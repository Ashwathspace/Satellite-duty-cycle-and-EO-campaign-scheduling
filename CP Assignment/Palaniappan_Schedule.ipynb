{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5IcYa5EByv0"
      },
      "source": [
        "# Scheduling Assignment: Palaniappan\n",
        "\n",
        "\n",
        "### Following items were requested as part of the assingment\n",
        "- Create a python script that takes in a data set of 40 most populas cities in the world\n",
        "\n",
        "- The program finds out the weather data of the city to estimate the ideal day for satellite imaging, given the following conditions are met\n",
        "  - The satellite can take 8 city images a day\n",
        "  - There are 7 days available to take images of all 40 cities\n",
        "  - All the cities must be covered\n",
        "  - It would be nice to present the data in a sorted manner and if possible visually\n",
        "\n",
        "Note: Orbit parameters and downlink capabilities must be ignored for this assignment\n",
        "\n",
        "Additionally, google collab or jupyter notebook is prefered.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEoJpN1mEO1P"
      },
      "source": [
        "Pip installing requests_html package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjHg5M-zYR0d",
        "outputId": "39d0a4cd-d142-4d9e-987c-7fe2ee01a7ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests_html in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.31.0)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.0.0)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.10/dist-packages (from requests_html) (1.5.1)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.10/dist-packages (from requests_html) (1.20.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (from requests_html) (0.0.2)\n",
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.1.2)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.0.0)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (2024.2.2)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (7.1.0)\n",
            "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (11.1.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.66.2)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.26.18)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (10.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests_html) (4.12.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests_html) (4.9.4)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests_html) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (3.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.18.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests_html) (4.11.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests_html) (2.5)\n"
          ]
        }
      ],
      "source": [
        "! pip install requests_html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdq4dOVxEyld"
      },
      "source": [
        "Getting the required libraries and importing required parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTM9WguA4q-I"
      },
      "outputs": [],
      "source": [
        "from requests_html import HTMLSession, AsyncHTMLSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYY1ga9fFCow"
      },
      "source": [
        "This part of the code gets the required libraries and loads in a dataset of 40 most populated cities. The dataset also includes the latitude and longitude values and population values. The population values are sorted in a descending manner.\n",
        "\n",
        "The source for the dataset is [Most Populated Cities](https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-populated-places/)\n",
        "\n",
        "The csv data file is then read into a data frame variable data and then split into 4 different df based on equal number of rows. This is done to avoid timeout or runtime errors that were commonly observed with this sort of data aquisition.\n",
        "\n",
        "a variable s defines the HTMLSession object that would be used later\n",
        "\n",
        "Apart from this today variable is also defined to do a forecast. This would mean that all the data would be latest available values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz7Db270raZn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "import requests\n",
        "import csv\n",
        "\n",
        "# Path to the population data\n",
        "path = \"/content/Top 40 Populated.csv\"\n",
        "\n",
        "# Load data from the CSV file into a pandas DataFrame\n",
        "data = pd.read_csv(path)\n",
        "df1 = data.iloc[0:10,:]\n",
        "df2 = data.iloc[10:20,:]\n",
        "df3 = data.iloc[20:30,:]\n",
        "df4 = data.iloc[30:40,:]\n",
        "\n",
        "\n",
        "s = HTMLSession()\n",
        "\n",
        "\n",
        "# Get today's date\n",
        "today = date.today()\n",
        "\n",
        "# Define a dictionary to map weekday numbers to their names\n",
        "weekdays = {\n",
        "  0: \"Monday\",\n",
        "  1: \"Tuesday\",\n",
        "  2: \"Wednesday\",\n",
        "  3: \"Thursday\",\n",
        "  4: \"Friday\",\n",
        "  5: \"Saturday\",\n",
        "  6: \"Sunday\"\n",
        "}\n",
        "\n",
        "# List to store the next 7 days as weekdays\n",
        "next_7_weekdays = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_ByC6R9Gnk7"
      },
      "source": [
        "Several methods were considered and tried for getting the weather data.\n",
        "\n",
        "One for the first trys was using APIs but each piece of code requires around 240 requests and many times that for testing. Most commercial api providers only provide 1000 requests per day for free. Also the api key needs to be part of the code which cannot be shared.\n",
        "\n",
        "In this api method, openweather and open-meteo were applied and found it to be working but not suitable for sharing and the scale of the project.\n",
        "\n",
        "Hence the alternate idea is being implemented here where the weather data is scrapped from the google website. Since this is an assignment for public use and not intended for profit, it does not violate any policy of fair use of public data..\n",
        "\n",
        "The program takes in the s variable defined in the previous code block to get the url request and later the elements regarding the weather description and presipitation are input to get the data.\n",
        "\n",
        "j and k are counter variables for the loop and get the forecast values.\n",
        "\n",
        "a new data frame adds values of weather and presipitation to the existing df.\n",
        "\n",
        "Finally a new output csv file is created out of this.\n",
        "\n",
        "A print l function runs from 0..1...2... 9 to give the status of the code\n",
        "\n",
        "**IMP - It is essential to change the headers value at 2 locations in the code  to the one of the current pc/ browser to make the code work, simply serach my user agent and add the value in place**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "zsE5zOjgvpZW",
        "outputId": "7a8b3223-452d-44c7-be60-2c56a8cc0d1e"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'find'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2363bb8fc804>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Change headers data here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'User-Agent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div.VQF4g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span#wob_dc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Not Found\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div.wtsRwe'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span#wob_pp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Not Found\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
          ]
        }
      ],
      "source": [
        "for l in range(10):\n",
        "\n",
        "  query = df1.iloc[l, 0]\n",
        "\n",
        "  url = f'https://www.google.com/search?q=weather+{query}+weather'\n",
        "\n",
        "  # Change headers data here\n",
        "  r = s.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'}, timeout = 10)\n",
        "  df1.iloc[l, 6] = r.html.find('div.VQF4g', first=True).find('span#wob_dc', first=True).text or \"Not Found\"\n",
        "  df1.iloc[l, 7] = r.html.find('div.wtsRwe', first=True).find('span#wob_pp', first=True).text or \"Not Found\"\n",
        "\n",
        "  j = 8\n",
        "  k = 9\n",
        "  # Loop through the next 7 days\n",
        "  for i in range(6):\n",
        "    # Get the date for the current loop iteration\n",
        "    next_date = today + timedelta(days=i)\n",
        "\n",
        "    # Use the weekday() method to get the weekday number (0-6)\n",
        "    weekday_number = next_date.weekday()\n",
        "\n",
        "    # Access the weekday name from the dictionary\n",
        "    weekday_name = weekdays[weekday_number]\n",
        "\n",
        "    # Add the weekday name to the list\n",
        "    next_7_weekdays.append(weekday_name)\n",
        "\n",
        "    url = f'https://www.google.com/search?q=weather+{query}+weather+on+{weekday_name}'\n",
        "\n",
        "    # Change headers data here\n",
        "    r = s.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'}, timeout = 10)\n",
        "    df1.iloc[l, j] = r.html.find('div.VQF4g', first=True).find('span#wob_dc', first=True).text\n",
        "    df1.iloc[l, k] = r.html.find('div.wtsRwe', first=True).find('span#wob_pp', first=True).text\n",
        "\n",
        "    j = j+2\n",
        "    k = k+2\n",
        "  print(l)\n",
        "\n",
        "# File path for output file# File path for output file\n",
        "file_path = '/content/output_data 1.csv'\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' (replace with your actual DataFrame name)\n",
        "df1.to_csv(file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kdtjxBiNfx1"
      },
      "source": [
        "The process is repeated for other data splits as well. This is done to avoid runtime error common with this type of programing. A runout time was also tried but didn't seem to work.\n",
        "\n",
        "**IMP - It is essential to change the headers value at 2 locations in the code  to the one of the current pc/ browser to make the code work, simply serach my user agent and add the value in place**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qgLs_c0wNZt"
      },
      "outputs": [],
      "source": [
        "for l in range(10):\n",
        "  query = df2.iloc[l, 0]\n",
        "  url = f'https://www.google.com/search?q=weather+{query}+weather'\n",
        "  r = s.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'})\n",
        "  df2.iloc[l, 6] = r.html.find('div.VQF4g', first=True).find('span#wob_dc', first=True).text\n",
        "  df2.iloc[l, 7] = r.html.find('div.wtsRwe', first=True).find('span#wob_pp', first=True).text\n",
        "\n",
        "  j = 8\n",
        "  k = 9\n",
        "  # Loop through the next 7 days\n",
        "  for i in range(6):\n",
        "    # Get the date for the current loop iteration\n",
        "    next_date = today + timedelta(days=i)\n",
        "\n",
        "    # Use the weekday() method to get the weekday number (0-6)\n",
        "    weekday_number = next_date.weekday()\n",
        "\n",
        "    # Access the weekday name from the dictionary\n",
        "    weekday_name = weekdays[weekday_number]\n",
        "\n",
        "    # Add the weekday name to the list\n",
        "    next_7_weekdays.append(weekday_name)\n",
        "\n",
        "    url = f'https://www.google.com/search?q=weather+{query}+weather+on+{weekday_name}'\n",
        "    r = s.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'})\n",
        "    df2.iloc[l, j] = r.html.find('div.VQF4g', first=True).find('span#wob_dc', first=True).text\n",
        "    df2.iloc[l, k] = r.html.find('div.wtsRwe', first=True).find('span#wob_pp', first=True).text\n",
        "    j = j+2\n",
        "    k = k+2\n",
        "  print(l)\n",
        "\n",
        "# File path for output file\n",
        "file_path = '/content/output_data 2.csv'\n",
        "# new_df = combine_rows(df1, df2)\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' (replace with your actual DataFrame name)\n",
        "df2.to_csv(file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSO29xvhN3C-"
      },
      "source": [
        "The process is repeated for other data splits as well. This is done to avoid runtime error common with this type of programing. A runout time was also tried but didn't seem to work.\n",
        "\n",
        "**IMP - It is essential to change the headers value at 2 locations in the code  to the one of the current pc/ browser to make the code work, simply serach my user agent and add the value in place**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW1L7sr6wrDD"
      },
      "outputs": [],
      "source": [
        "for l in range(10):\n",
        "  query = df3.iloc[l, 0]\n",
        "  url = f'https://www.google.com/search?q=weather+{query}+weather'\n",
        "  r = s.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'})\n",
        "  df3.iloc[l, 6] = r.html.find('div.VQF4g', first=True).find('span#wob_dc', first=True).text\n",
        "  df3.iloc[l, 7] = r.html.find('div.wtsRwe', first=True).find('span#wob_pp', first=True).text\n",
        "\n",
        "  j = 8\n",
        "  k = 9\n",
        "  # Loop through the next 7 days\n",
        "  for i in range(6):\n",
        "    # Get the date for the current loop iteration\n",
        "    next_date = today + timedelta(days=i)\n",
        "\n",
        "    # Use the weekday() method to get the weekday number (0-6)\n",
        "    weekday_number = next_date.weekday()\n",
        "\n",
        "    # Access the weekday name from the dictionary\n",
        "    weekday_name = weekdays[weekday_number]\n",
        "\n",
        "    # Add the weekday name to the list\n",
        "    next_7_weekdays.append(weekday_name)\n",
        "\n",
        "    url = f'https://www.google.com/search?q=weather+{query}+weather+on+{weekday_name}'\n",
        "    r = s.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'})\n",
        "    df3.iloc[l, j] = r.html.find('div.VQF4g', first=True).find('span#wob_dc', first=True).text\n",
        "    df3.iloc[l, k] = r.html.find('div.wtsRwe', first=True).find('span#wob_pp', first=True).text\n",
        "    j = j+2\n",
        "    k = k+2\n",
        "  print(l)\n",
        "\n",
        "# File path for output file\n",
        "file_path = '/content/output_data 3.csv'\n",
        "\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' (replace with your actual DataFrame name)\n",
        "df3.to_csv(file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p51Jg0RsN6O-"
      },
      "source": [
        "The process is repeated for other data splits as well. This is done to avoid runtime error common with this type of programing. A runout time was also tried but didn't seem to work.\n",
        "\n",
        "**IMP - It is essential to change the headers value at 2 locations in the code  to the one of the current pc/ browser to make the code work, simply serach my user agent and add the value in place**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn_urJHVyf3X"
      },
      "outputs": [],
      "source": [
        "for l in range(10):\n",
        "  query = df4.iloc[l, 0]\n",
        "  url = f'https://www.google.com/search?q=weather+{query}+weather'\n",
        "  r = s.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'})\n",
        "  df4.iloc[l, 6] = r.html.find('div.VQF4g', first=True).find('span#wob_dc', first=True).text\n",
        "  df4.iloc[l, 7] = r.html.find('div.wtsRwe', first=True).find('span#wob_pp', first=True).text\n",
        "\n",
        "  j = 8\n",
        "  k = 9\n",
        "  # Loop through the next 7 days\n",
        "  for i in range(6):\n",
        "    # Get the date for the current loop iteration\n",
        "    next_date = today + timedelta(days=i)\n",
        "\n",
        "    # Use the weekday() method to get the weekday number (0-6)\n",
        "    weekday_number = next_date.weekday()\n",
        "\n",
        "    # Access the weekday name from the dictionary\n",
        "    weekday_name = weekdays[weekday_number]\n",
        "\n",
        "    # Add the weekday name to the list\n",
        "    next_7_weekdays.append(weekday_name)\n",
        "\n",
        "    url = f'https://www.google.com/search?q=weather+{query}+weather+on+{weekday_name}'\n",
        "    r = s.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'})\n",
        "    df4.iloc[l, j] = r.html.find('div.VQF4g', first=True).find('span#wob_dc', first=True).text\n",
        "    df4.iloc[l, k] = r.html.find('div.wtsRwe', first=True).find('span#wob_pp', first=True).text\n",
        "    j = j+2\n",
        "    k = k+2\n",
        "  print(l)\n",
        "\n",
        "# File path for output file\n",
        "file_path = '/content/output_data 4.csv'\n",
        "\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' (replace with your actual DataFrame name)\n",
        "df4.to_csv(file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehGgpkqDOVSG"
      },
      "source": [
        "Now the 4 copes of output data are merged together by concatenate to obtain one single file. It shsould be noted here that one location in the cities \"Chongqing\" was not possible to query online for some reason and hence manual data has been entered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFnjQYREzXIc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define paths to your CSV files (replace with your actual paths)\n",
        "csv_files = [\n",
        "    \"/content/output_data 1.csv\",\n",
        "    \"/content/output_data 2.csv\",\n",
        "    \"/content/output_data 3.csv\",\n",
        "    \"/content/output_data 4.csv\"\n",
        "]\n",
        "\n",
        "# Empty list to store DataFrames\n",
        "all_data = []\n",
        "\n",
        "# Read each CSV file into a separate DataFrame\n",
        "for filename in csv_files:\n",
        "     df = pd.read_csv(filename)\n",
        "     all_data.append(df)\n",
        "\n",
        "     # Concatenate the DataFrames vertically (one below another)\n",
        "     merged_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "     # Optional: Specify output filename (replace with your desired name)\n",
        "     output_file = \"/content/merged_data.csv\"\n",
        "\n",
        "     # Save the merged DataFrame to a new CSV file\n",
        "     merged_df.to_csv(output_file, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlOqAaTYOxnw"
      },
      "source": [
        "## Work in progress (bug fixes)\n",
        "\n",
        "The data aquisiton part of the code took way longer than expected since conventional methods are not used for above mentioned reasons. Hence, this part of the code is still work in progress and I would be happy to explain the logic here and would fix the bugs at a later stage if agreed upon.\n",
        "\n",
        "This is what the currrent data set looks like -\n",
        "\n",
        "![Current dataset](/content/dataset.png)\n",
        "\n",
        "The high ranked data are marked green, followed by yellow and orange.\n",
        "\n",
        "The weather description from google is divided into three types or ranks.\n",
        "  - The first rank includes, which includes following key words \"Clear\", \"Mostly sunny\", \"Clear with periodic clouds\", \"Sunny\" These are ideal conditions for satellite imagery\n",
        "\n",
        "  - The second rank includes keywords like \"Partly cloudy\", \"Haze\", \"Light rain showers\", \"Cloudy periodically clear\" These are not the worst conditions for the imaging\n",
        "\n",
        "  - The third rank includes the rest\n",
        "\n",
        "\n",
        "## Algorithm\n",
        " - Each city is picked in a sorted manner where the city with highest population is first and a horizontal comparitior to check if the current rank is lower than previous rank, if yes then it switches. Essentially giving the best options for the more populous cities\n",
        "\n",
        " - Parallely a second loop makes sure as the data fills up in each day, it cannot exceede beyond 8\n",
        "\n",
        "Essentially the code looks like this\n",
        "\n",
        "for cities in (1:40)\n",
        "  \n",
        "  for days in (1:7)\n",
        "\n",
        "    if (days[] <= 8)\n",
        "\n",
        "      if compare previous value set for weather for that city has a higher or a lower value then change it accordingly\n",
        "\n",
        "      else do nothing, this skips the day once the tank is full\n",
        "\n",
        "\n",
        "Finally codes would be checked but this should work based on the algorithm but two additional days would serve better to finish the problem.\n",
        "\n",
        "\n",
        "## Logic\n",
        "\n",
        "The reason for selecting this algorithm is that higher priority is given to more populous cities. The code takes in the first city from a sorted (by population) database of cities takes in the first day out of 7 and checks if the first day weather is better than the one that it has stored (which is initialized to be same) then it does the same for other days. Parallely it has a an empty set for each day in the begining that it adds city names to and once it hits 8 it goes into else statement which basically skips the day.\n",
        "\n",
        "The benefits of this logic are -\n",
        "  - More populous cities are given a higher priority\n",
        "  - All the days for a city is studied prior to assigning the city to one day\n",
        "  - The weather data is sorted into broader groups for ranking which means there is some room change in weather data.\n",
        "  - The sequence of check for each days goes from the closest day to the later days. This means priority is given to days which are closer to the day where the weather data was checked and later days forecast could change, where the sequence of cities can be altered.\n",
        "\n",
        "It is recommended to run this code daily so as to change the sequence based on change in forecast.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQHFfWQTD2QV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "merg = pd.read_csv('/content/merged_data.csv')\n",
        "# Replace with actual city names\n",
        "city_list = merg.iloc[:,0]\n",
        "\n",
        "# Weather data structure (replace with actual data retrieval logic)\n",
        "\n",
        "weather_data = {}\n",
        "for i in range(40):\n",
        "  city = merg.iloc[i,0]\n",
        "  weather_data[city] = merg.iloc[i,6:19:2]\n",
        "\n",
        "# print(weather_data)\n",
        "\n",
        "def get_weather_rank(weather):\n",
        "  # Replace with your ranking logic (refer to previous explanation)\n",
        "  if weather in [\"Clear\", \"Mostly sunny\", \"Clear with periodic clouds\", \"Sunny\"]:\n",
        "    return 3\n",
        "  elif weather in [\"Partly cloudy\", \"Haze\", \"Light rain showers\", \"Cloudy periodically clear\"]:\n",
        "    return 2\n",
        "  else:\n",
        "    return 1  # Not ideal\n",
        "\n",
        "\n",
        "Day = {}\n",
        "\n",
        "for i in range(0,40):\n",
        "  for j in range(1,8):\n",
        "    if len(Day[j]) <= 8:\n",
        "      if get_weather_rank(merg.iloc[i,j]) >= get_weather_rank(merg.iloc[i,j]):\n",
        "        Day[j] =  [merg.iloc[i,0]]\n",
        "      else:\n",
        "        break\n",
        "    else:\n",
        "      break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
